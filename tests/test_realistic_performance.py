"""
Realistic performance test for video generation with detailed metrics
Tests with real-world scenario: 10 slides, 2-5 minutes total duration
"""
import asyncio
import json
import os
import logging
import psutil
import time
from datetime import datetime
from src.services.video_generator import VideoGenerator
from src.services.tts_service import estimate_speech_duration

logging.basicConfig(level=logging.INFO)

def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

async def test_realistic_performance():
    """Test with realistic lesson data - 10 slides, 2-5 minutes total"""
    
    # Real-world lesson data with longer TTS scripts
    lesson_data = {
        "slides": [
            {
                "slide_id": 1,
                "title": "Gi·ªõi thi·ªáu v·ªÅ Machine Learning",
                "content": [
                    "Machine Learning l√† g√¨?",
                    "·ª®ng d·ª•ng trong ƒë·ªùi s·ªëng",
                    "C√°c lo·∫°i ML: Supervised, Unsupervised, Reinforcement"
                ],
                "tts_script": "Ch√†o m·ª´ng c√°c b·∫°n ƒë·∫øn v·ªõi kh√≥a h·ªçc Machine Learning cƒÉn b·∫£n. Machine Learning hay H·ªçc m√°y l√† m·ªôt nh√°nh c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o cho ph√©p m√°y t√≠nh h·ªçc v√† ƒë∆∞a ra quy·∫øt ƒë·ªãnh t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn l·∫≠p tr√¨nh c·ª• th·ªÉ cho t·ª´ng t√°c v·ª•. Trong cu·ªôc s·ªëng h√†ng ng√†y, ch√∫ng ta th∆∞·ªùng xuy√™n ti·∫øp x√∫c v·ªõi Machine Learning qua c√°c h·ªá th·ªëng g·ª£i √Ω tr√™n Netflix, nh·∫≠n di·ªán khu√¥n m·∫∑t tr√™n Facebook, hay tr·ª£ l√Ω ·∫£o nh∆∞ Siri v√† Google Assistant. Machine Learning ƒë∆∞·ª£c chia th√†nh ba lo·∫°i ch√≠nh: Supervised Learning h·ªçc t·ª´ d·ªØ li·ªáu c√≥ nh√£n, Unsupervised Learning t√¨m ki·∫øm pattern trong d·ªØ li·ªáu kh√¥ng c√≥ nh√£n, v√† Reinforcement Learning h·ªçc th√¥ng qua vi·ªác t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng v√† nh·∫≠n ph·∫ßn th∆∞·ªüng.",
                "image_keywords": ["machine learning", "AI", "artificial intelligence", "technology"]
            },
            {
                "slide_id": 2,
                "title": "D·ªØ li·ªáu v√† Ti·ªÅn x·ª≠ l√Ω",
                "content": [
                    "T·∫ßm quan tr·ªçng c·ªßa d·ªØ li·ªáu",
                    "L√†m s·∫°ch d·ªØ li·ªáu",
                    "Feature Engineering",
                    "Train/Validation/Test Split"
                ],
                "tts_script": "D·ªØ li·ªáu l√† y·∫øu t·ªë quan tr·ªçng nh·∫•t trong Machine Learning, c√≥ c√¢u n√≥i r·∫±ng 'garbage in, garbage out' - d·ªØ li·ªáu k√©m ch·∫•t l∆∞·ª£ng s·∫Ω cho ra m√¥ h√¨nh k√©m ch·∫•t l∆∞·ª£ng. Qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu bao g·ªìm l√†m s·∫°ch d·ªØ li·ªáu b·∫±ng c√°ch lo·∫°i b·ªè gi√° tr·ªã thi·∫øu, x·ª≠ l√Ω outliers, v√† chu·∫©n h√≥a format. Feature Engineering l√† ngh·ªá thu·∫≠t t·∫°o ra c√°c ƒë·∫∑c tr∆∞∆°ng m·ªõi t·ª´ d·ªØ li·ªáu th√¥ ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh. Cu·ªëi c√πng, ch√∫ng ta c·∫ßn chia d·ªØ li·ªáu th√†nh ba t·∫≠p: Training ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh, Validation ƒë·ªÉ tinh ch·ªânh hyperparameters, v√† Test ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t cu·ªëi c√πng. Vi·ªác chia d·ªØ li·ªáu ƒë√∫ng c√°ch gi√∫p tr√°nh overfitting v√† ƒë·∫£m b·∫£o m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët v·ªõi d·ªØ li·ªáu m·ªõi.",
                "image_keywords": ["data processing", "data cleaning", "feature engineering", "dataset"]
            },
            {
                "slide_id": 3,
                "title": "Linear Regression",
                "content": [
                    "Kh√°i ni·ªám c∆° b·∫£n",
                    "Cost Function",
                    "Gradient Descent",
                    "V√≠ d·ª• th·ª±c t·∫ø"
                ],
                "tts_script": "Linear Regression l√† thu·∫≠t to√°n Machine Learning c∆° b·∫£n nh·∫•t, ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c d·ª±a tr√™n m·ªëi quan h·ªá tuy·∫øn t√≠nh gi·ªØa input v√† output. M√¥ h√¨nh n√†y t√¨m c√°ch v·∫Ω m·ªôt ƒë∆∞·ªùng th·∫≥ng t·ªët nh·∫•t qua c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë·ªÉ minimizing sai s·ªë. Cost Function, th∆∞·ªùng l√† Mean Squared Error, ƒëo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh b·∫±ng c√°ch t√≠nh to√°n sai kh√°c gi·ªØa gi√° tr·ªã th·ª±c v√† gi√° tr·ªã d·ª± ƒëo√°n. Gradient Descent l√† thu·∫≠t to√°n optimization ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ra c√°c tham s·ªë t·ªëi ∆∞u cho m√¥ h√¨nh b·∫±ng c√°ch di chuy·ªÉn theo h∆∞·ªõng gi·∫£m cost function. M·ªôt v√≠ d·ª• th·ª±c t·∫ø c·ªßa Linear Regression l√† d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n di·ªán t√≠ch, s·ªë ph√≤ng ng·ªß, v√† v·ªã tr√≠. ƒê√¢y l√† n·ªÅn t·∫£ng ƒë·ªÉ hi·ªÉu c√°c thu·∫≠t to√°n ph·ª©c t·∫°p h∆°n.",
                "image_keywords": ["linear regression", "mathematics", "graph", "prediction"]
            },
            {
                "slide_id": 4,
                "title": "Classification v√† Decision Trees",
                "content": [
                    "B√†i to√°n ph√¢n lo·∫°i",
                    "Decision Trees ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o",
                    "Entropy v√† Information Gain",
                    "Overfitting v√† Pruning"
                ],
                "tts_script": "Classification l√† lo·∫°i b√†i to√°n Machine Learning d·ª± ƒëo√°n nh√£n ho·∫∑c danh m·ª•c cho d·ªØ li·ªáu ƒë·∫ßu v√†o, kh√°c v·ªõi regression d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c. Decision Trees l√† m·ªôt thu·∫≠t to√°n classification tr·ª±c quan, ho·∫°t ƒë·ªông gi·ªëng nh∆∞ vi·ªác ƒë∆∞a ra quy·∫øt ƒë·ªãnh trong ƒë·ªùi th·ª±c b·∫±ng c√°ch ƒë·∫∑t m·ªôt chu·ªói c√¢u h·ªèi yes/no. Entropy ƒëo l∆∞·ªùng ƒë·ªô h·ªón lo·∫°n trong d·ªØ li·ªáu, c√≤n Information Gain cho bi·∫øt m·ªôt feature c√≥ kh·∫£ nƒÉng ph√¢n chia d·ªØ li·ªáu t·ªët nh∆∞ th·∫ø n√†o. Decision Trees d·ªÖ b·ªã overfitting khi c√¢y qu√° s√¢u v√† ph·ª©c t·∫°p, d·∫´n ƒë·∫øn hi·ªáu su·∫•t k√©m tr√™n d·ªØ li·ªáu m·ªõi. Pruning l√† k·ªπ thu·∫≠t c·∫Øt b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o ra m√¥ h√¨nh ƒë∆°n gi·∫£n v√† t·ªïng qu√°t h√≥a t·ªët h∆°n. V√≠ d·ª• th·ª±c t·∫ø l√† ph√¢n lo·∫°i email spam d·ª±a tr√™n t·ª´ kh√≥a v√† ng∆∞·ªùi g·ª≠i.",
                "image_keywords": ["decision tree", "classification", "flowchart", "data science"]
            },
            {
                "slide_id": 5,
                "title": "Random Forest v√† Ensemble Methods",
                "content": [
                    "Wisdom of Crowds",
                    "Random Forest Algorithm",
                    "Bagging vs Boosting",
                    "Feature Importance"
                ],
                "tts_script": "Ensemble Methods d·ª±a tr√™n nguy√™n l√Ω 'wisdom of crowds' - nhi·ªÅu m√¥ h√¨nh y·∫øu k·∫øt h·ª£p l·∫°i c√≥ th·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh m·∫°nh. Random Forest l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n ensemble ph·ªï bi·∫øn nh·∫•t, k·∫øt h·ª£p nhi·ªÅu Decision Trees ƒë∆∞·ª£c training tr√™n c√°c subset kh√°c nhau c·ªßa d·ªØ li·ªáu. Bagging nh∆∞ Random Forest training c√°c m√¥ h√¨nh song song v√† l·∫•y trung b√¨nh k·∫øt qu·∫£, trong khi Boosting nh∆∞ AdaBoost training tu·∫ßn t·ª± v√† t·∫≠p trung v√†o c√°c m·∫´u kh√≥ ph√¢n lo·∫°i. Random Forest cung c·∫•p Feature Importance, cho bi·∫øt feature n√†o quan tr·ªçng nh·∫•t trong vi·ªác ƒë∆∞a ra d·ª± ƒëo√°n. ∆Øu ƒëi·ªÉm c·ªßa Random Forest l√† robust v·ªõi noise, √≠t b·ªã overfitting, v√† ho·∫°t ƒë·ªông t·ªët m√† kh√¥ng c·∫ßn tuning hyperparameters nhi·ªÅu. Thu·∫≠t to√°n n√†y th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m baseline trong c√°c competition Machine Learning.",
                "image_keywords": ["random forest", "ensemble", "trees", "algorithm"]
            },
            {
                "slide_id": 6,
                "title": "Support Vector Machines (SVM)",
                "content": [
                    "T√¨m ƒë∆∞·ªùng ph√¢n chia t·ªëi ∆∞u",
                    "Support Vectors",
                    "Kernel Trick",
                    "SVM cho Classification v√† Regression"
                ],
                "tts_script": "Support Vector Machine l√† thu·∫≠t to√°n m·∫°nh m·∫Ω t√¨m c√°ch ph√¢n chia d·ªØ li·ªáu b·∫±ng m·ªôt ƒë∆∞·ªùng th·∫≥ng ho·∫∑c si√™u ph·∫≥ng sao cho kho·∫£ng c√°ch ƒë·∫øn c√°c ƒëi·ªÉm g·∫ßn nh·∫•t l√† l·ªõn nh·∫•t. Nh·ªØng ƒëi·ªÉm d·ªØ li·ªáu g·∫ßn ƒë∆∞·ªùng ph√¢n chia nh·∫•t ƒë∆∞·ª£c g·ªçi l√† Support Vectors, ch√∫ng quy·∫øt ƒë·ªãnh v·ªã tr√≠ c·ªßa ƒë∆∞·ªùng ph√¢n chia. Kernel Trick cho ph√©p SVM x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng tuy·∫øn t√≠nh b·∫±ng c√°ch mapping d·ªØ li·ªáu l√™n kh√¥ng gian c√≥ chi·ªÅu cao h∆°n m√† kh√¥ng c·∫ßn t√≠nh to√°n explicit. C√°c kernel ph·ªï bi·∫øn include linear, polynomial, v√† RBF (Radial Basis Function). SVM c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c·∫£ classification v√† regression (SVR), v·ªõi ∆∞u ƒëi·ªÉm l√† ho·∫°t ƒë·ªông t·ªët v·ªõi d·ªØ li·ªáu c√≥ nhi·ªÅu features v√† √≠t samples. Tuy nhi√™n, SVM c√≥ th·ªÉ ch·∫≠m v·ªõi dataset l·ªõn v√† kh√≥ interpret k·∫øt qu·∫£.",
                "image_keywords": ["SVM", "support vector machine", "classification", "margin"]
            },
            {
                "slide_id": 7,
                "title": "Neural Networks C∆° b·∫£n",
                "content": [
                    "T·ª´ Perceptron ƒë·∫øn Multi-layer",
                    "Activation Functions",
                    "Backpropagation",
                    "Deep Learning Introduction"
                ],
                "tts_script": "Neural Networks l·∫•y c·∫£m h·ª©ng t·ª´ c√°ch ho·∫°t ƒë·ªông c·ªßa n√£o b·ªô, b·∫Øt ƒë·∫ßu t·ª´ Perceptron ƒë∆°n gi·∫£n ƒë·∫øn Multi-layer Perceptron ph·ª©c t·∫°p. M·ªói neuron nh·∫≠n inputs, nh√¢n v·ªõi weights, c·ªông bias, v√† ƒëi qua activation function ƒë·ªÉ t·∫°o output. Activation functions nh∆∞ sigmoid, tanh, v√† ReLU quy·∫øt ƒë·ªãnh output c·ªßa neuron c√≥ ƒë∆∞·ª£c activate hay kh√¥ng. Backpropagation l√† thu·∫≠t to√°n training neural networks b·∫±ng c√°ch t√≠nh gradient v√† update weights theo h∆∞·ªõng gi·∫£m loss function t·ª´ output layer v·ªÅ input layer. Deep Learning l√† ph·∫ßn m·ªü r·ªông c·ªßa neural networks v·ªõi nhi·ªÅu hidden layers, cho ph√©p h·ªçc c√°c pattern ph·ª©c t·∫°p trong d·ªØ li·ªáu nh∆∞ h√¨nh ·∫£nh, text, v√† audio. Neural networks ƒë·∫∑c bi·ªát m·∫°nh trong vi·ªác h·ªçc non-linear relationships v√† c√≥ th·ªÉ approximate b·∫•t k·ª≥ function n√†o v·ªõi ƒë·ªß neurons. ƒê√¢y l√† n·ªÅn t·∫£ng cho c√°c breakthrough recent trong AI.",
                "image_keywords": ["neural network", "deep learning", "neurons", "AI brain"]
            },
            {
                "slide_id": 8,
                "title": "Clustering v√† K-Means",
                "content": [
                    "Unsupervised Learning",
                    "K-Means Algorithm",
                    "Choosing K value",
                    "Hierarchical Clustering"
                ],
                "tts_script": "Clustering l√† lo·∫°i Unsupervised Learning t√¨m c√°ch nh√≥m d·ªØ li·ªáu th√†nh c√°c clusters d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng m√† kh√¥ng c·∫ßn labels. K-Means l√† thu·∫≠t to√°n clustering ph·ªï bi·∫øn nh·∫•t, ho·∫°t ƒë·ªông b·∫±ng c√°ch kh·ªüi t·∫°o K centroids ng·∫´u nhi√™n, assign each data point to nearest centroid, sau ƒë√≥ update centroids l√† trung b√¨nh c·ªßa cluster. Process n√†y l·∫∑p l·∫°i until convergence. Vi·ªác ch·ªçn K value ƒë√∫ng r·∫•t quan tr·ªçng, c√≥ th·ªÉ s·ª≠ d·ª•ng Elbow Method ho·∫∑c Silhouette Analysis ƒë·ªÉ t√¨m K optimal. Hierarchical Clustering t·∫°o ra c√¢y ph√¢n c·∫•p c√°c clusters, c√≥ th·ªÉ l√† agglomerative (bottom-up) ho·∫∑c divisive (top-down). Clustering applications include customer segmentation, image segmentation, market research, v√† data compression. ∆Øu ƒëi·ªÉm c·ªßa clustering l√† kh√¥ng c·∫ßn labeled data v√† c√≥ th·ªÉ reveal hidden patterns trong d·ªØ li·ªáu.",
                "image_keywords": ["clustering", "k-means", "data groups", "unsupervised"]
            },
            {
                "slide_id": 9,
                "title": "Model Evaluation v√† Metrics",
                "content": [
                    "Accuracy, Precision, Recall",
                    "F1-Score v√† Confusion Matrix",
                    "ROC Curve v√† AUC",
                    "Cross-Validation"
                ],
                "tts_script": "Model evaluation l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh Machine Learning. Accuracy ƒëo t·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng t·ªïng th·ªÉ, nh∆∞ng c√≥ th·ªÉ misleading v·ªõi imbalanced dataset. Precision ƒëo t·ª∑ l·ªá positive predictions th·ª±c s·ª± ƒë√∫ng, c√≤n Recall ƒëo t·ª∑ l·ªá actual positives ƒë∆∞·ª£c detect correctly. F1-Score l√† harmonic mean c·ªßa Precision v√† Recall, c√¢n b·∫±ng gi·ªØa hai metrics n√†y. Confusion Matrix visualization c√°c lo·∫°i prediction errors, gi√∫p understand model behavior better. ROC Curve plot True Positive Rate vs False Positive Rate, v√† AUC (Area Under Curve) summarize performance trong m·ªôt s·ªë duy nh·∫•t. Cross-Validation chia d·ªØ li·ªáu th√†nh k folds v√† training k times ƒë·ªÉ c√≥ estimation robust c·ªßa model performance. Vi·ªác ch·ªçn ƒë√∫ng evaluation metrics depends on business objectives v√† nature c·ªßa problem ƒë∆∞·ª£c gi·∫£i quy·∫øt.",
                "image_keywords": ["evaluation", "metrics", "accuracy", "model performance"]
            },
            {
                "slide_id": 10,
                "title": "Deployment v√† Production",
                "content": [
                    "Model Serialization",
                    "API Creation",
                    "Monitoring v√† Maintenance",
                    "MLOps Basics"
                ],
                "tts_script": "Deployment l√† b∆∞·ªõc cu·ªëi c√πng ƒë∆∞a m√¥ h√¨nh Machine Learning t·ª´ development environment l√™n production ƒë·ªÉ users c√≥ th·ªÉ s·ª≠ d·ª•ng. Model Serialization s·ª≠ d·ª•ng tools nh∆∞ pickle, joblib, ho·∫∑c ONNX ƒë·ªÉ save trained models th√†nh files c√≥ th·ªÉ load l·∫°i. API Creation th∆∞·ªùng s·ª≠ d·ª•ng frameworks nh∆∞ Flask, FastAPI, ho·∫∑c Django REST ƒë·ªÉ t·∫°o endpoints cho model inference. Monitoring production models r·∫•t quan tr·ªçng ƒë·ªÉ detect model drift khi data distribution thay ƒë·ªïi theo th·ªùi gian, c·∫ßn track metrics nh∆∞ accuracy, latency, v√† resource usage. MLOps l√† practice k·∫øt h·ª£p Machine Learning v·ªõi DevOps, automation model training, testing, deployment, v√† monitoring. Version control cho models, automated testing, CI/CD pipelines, v√† infrastructure as code l√† essential components c·ªßa MLOps. Successful deployment requires collaboration gi·ªØa data scientists, software engineers, v√† DevOps teams ƒë·ªÉ ensure models are reliable, scalable, v√† maintainable in production environment.",
                "image_keywords": ["deployment", "production", "API", "MLOps", "server"]
            }
        ]
    }
    
    # Calculate expected metrics
    total_estimated_duration = sum(estimate_speech_duration(slide['tts_script']) for slide in lesson_data['slides'])
    
    video_generator = VideoGenerator()
    output_path = f"realistic_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
    
    # Detailed timing tracking
    phase_times = {}
    memory_tracking = []
    
    def track_memory():
        memory_tracking.append({
            'time': time.time(),
            'memory_mb': get_memory_usage()
        })
    
    try:
        print("üöÄ REALISTIC PERFORMANCE TEST")
        print("=" * 50)
        print(f"üìä Test Configuration:")
        print(f"   - Slides: {len(lesson_data['slides'])}")
        print(f"   - Estimated total duration: {total_estimated_duration:.1f} seconds ({total_estimated_duration/60:.1f} minutes)")
        print(f"   - Average TTS length: {sum(len(s['tts_script']) for s in lesson_data['slides']) / len(lesson_data['slides']):.0f} characters")
        print(f"   - Output: {output_path}")
        print()
        
        track_memory()
        overall_start = time.time()
        
        print("üé¨ Starting video generation...")
        
        # Phase 1: Initialization
        phase_start = time.time()
        # Video generator is already initialized
        phase_times['initialization'] = time.time() - phase_start
        track_memory()
        
        # Phase 2: Main generation
        phase_start = time.time()
        result = await video_generator.generate_lesson_video(lesson_data, output_path)
        phase_times['video_generation'] = time.time() - phase_start
        track_memory()
        
        total_time = time.time() - overall_start
        
        # Analyze results
        print("\n‚úÖ GENERATION COMPLETED")
        print("=" * 50)
        
        # Timing analysis
        print("‚è±Ô∏è  Timing Analysis:")
        print(f"   - Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
        print(f"   - Time per slide: {total_time/len(lesson_data['slides']):.1f} seconds")
        print(f"   - Speed ratio: {total_estimated_duration/total_time:.2f}x (audio duration / generation time)")
        
        for phase, duration in phase_times.items():
            print(f"   - {phase.title()}: {duration:.1f}s ({duration/total_time*100:.1f}%)")
        
        # Memory analysis
        if memory_tracking:
            memories = [m['memory_mb'] for m in memory_tracking]
            print(f"\nüíæ Memory Analysis:")
            print(f"   - Initial: {memories[0]:.1f} MB")
            print(f"   - Peak: {max(memories):.1f} MB")
            print(f"   - Final: {memories[-1]:.1f} MB")
            print(f"   - Memory increase: {max(memories) - memories[0]:.1f} MB")
            print(f"   - Memory per slide: {(max(memories) - memories[0])/len(lesson_data['slides']):.1f} MB")
        
        # File analysis
        if os.path.exists(result):
            file_size_mb = os.path.getsize(result) / (1024 * 1024)
            print(f"\nüìÅ Output Analysis:")
            print(f"   - File size: {file_size_mb:.1f} MB")
            print(f"   - Compression ratio: {file_size_mb/total_estimated_duration:.2f} MB/second")
            print(f"   - File exists: ‚úÖ")
        else:
            print(f"\n‚ùå Output file not found: {result}")
            return False
        
        # Performance rating
        print(f"\nüéØ Performance Rating:")
        
        # Speed metrics
        slides_per_minute = len(lesson_data['slides']) / (total_time / 60)
        content_speed = total_estimated_duration / total_time  # How many seconds of content per second of processing
        
        print(f"   - Processing speed: {slides_per_minute:.1f} slides/minute")
        print(f"   - Content generation rate: {content_speed:.2f}x real-time")
        
        # Quality metrics
        if content_speed >= 1.5:  # Faster than 1.5x real-time
            speed_rating = "üåü Excellent"
        elif content_speed >= 1.0:
            speed_rating = "‚úÖ Good"
        elif content_speed >= 0.5:
            speed_rating = "‚ö†Ô∏è Acceptable"
        else:
            speed_rating = "‚ùå Slow"
            
        if memories and (max(memories) - memories[0]) < 500:  # Less than 500MB increase
            memory_rating = "üåü Excellent"
        elif memories and (max(memories) - memories[0]) < 1000:
            memory_rating = "‚úÖ Good"
        else:
            memory_rating = "‚ö†Ô∏è High memory usage"
        
        print(f"   - Speed: {speed_rating}")
        print(f"   - Memory efficiency: {memory_rating}")
        
        # Recommendations
        print(f"\nüí° Recommendations:")
        if content_speed < 1.0:
            print("   - Consider optimizing image processing")
            print("   - Check TTS service response time")
            print("   - Reduce concurrent processing if memory constrained")
        
        if memories and (max(memories) - memories[0]) > 800:
            print("   - Consider processing slides in smaller batches")
            print("   - Optimize image resolution/size")
            print("   - Add more aggressive garbage collection")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_realistic_performance())
    print("\n" + "="*50)
    print("üéâ TEST COMPLETED SUCCESSFULLY!" if success else "üí• TEST FAILED!")
    print("="*50)
